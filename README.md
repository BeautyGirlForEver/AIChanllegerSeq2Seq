# 赛题描述

## 简介

英中机器文本翻译作为此次比赛的任务之一，目标是评测各个团队机器翻译的能力。本次机器翻译语言方向为英文到中文。测试文本为口语领域数据。参赛队伍需要根据评测方提供的数据训练机器翻译系统，可以自由的选择机器翻译技术。例如，基于规则的翻译技术、统计机器翻译及神经网络机器翻译等。参赛队伍可以使用系统融合技术，但是系统融合系统不参与排名。需要指出，神经网络机器翻译常见的Ensemble方法，本次评测不认定为系统融合技术。

## 数据说明

我们将所有数据分割成为训练集、验证集和测试集合。我们提供了1000万左右英中对照的句子对作为数据集合。其中，训练集合占据绝大部分，验证集合8000对，测试集A 8000条，测试集B 8000条。训练数据主要来源于英语学习网站和电影字幕，领域为口语领域。所有双语句对经过人工检查，数据集从规模、相关度、质量上都有保障。一个英中对照的句子对，包含一句英文和一句中文文本，中文句子由英文句子人工翻译而成。中英文句子分别保存到两个文件中，两个文件中的中英文句子以行号形成一一对应的关系。验证集和测试集最终是以标准的XML格式发布给参赛方。

## 训练条件

本次评测只允许参赛方使用使用评测方指定的数据训练机器翻译系统，并对其排名。参赛方需遵守以下关于训练方式的说明。参赛方可以使用基本的自然语言处理工具，例如中文分词和命名实体识别。

## 结果提交说明

选手返回的结果需要采用指定的XML格式，系统最终会利用BLEU计算脚本来计算提交结果的BLEU得分。我们会提供将翻译结果转换为xml格式的转换脚本，具体调用方式如下：

格式：

```
wrap_xml.pl LANGUAGE SOURCE_XML_FILE SYSTEM_NAME < IN > OUT
```

例子:

```
wrap_xml.pl en test.en.sgm Toy < decoder-output > decoder-output.xml
```

## 评价标准

对于文本机器翻译，我们采用多种机器翻译自动评价指标。机器评价指标包括BLEU、 NIST score和TER，其中BLEU得分会作为主要的机器评价指标。英中机器翻译指标会采用基于字符（character-based）的评价方式，中文句子会被切分成单个汉字，翻译结果中的数字、英文等则不切分，然后再使用机器测试指标测试效果。所有的自动评测均采用大小写敏感（case-sensitive）的方式。

